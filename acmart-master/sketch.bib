@article{pix2pix,
abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
archivePrefix = {arXiv},
arxivId = {1611.07004},
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
eprint = {1611.07004},
file = {:F$\backslash$:/Hang/papers/Deep Learning/Generative/2016{\_}Image-to-Image Translation with Conditional Adversarial Networks{\_}Isola et al.pdf:pdf},
mendeley-groups = {Generative/Conditional},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}},
url = {http://arxiv.org/abs/1611.07004},
year = {2016}
}

@article{UNIT,
	abstract = {It's useful to automatically transform an image from its original form to some synthetic form (style, partial contents, etc.), while keeping the original structure or semantics. We define this requirement as the "image-to-image translation" problem, and propose a general approach to achieve it, based on deep convolutional and conditional generative adversarial networks (GANs), which has gained a phenomenal success to learn mapping images from noise input since 2014. In this work, we develop a two step (unsupervised) learning method to translate images between different domains by using unlabeled images without specifying any correspondence between them, so that to avoid the cost of acquiring labeled data. Compared with prior works, we demonstrated the capacity of generality in our model, by which variance of translations can be conduct by a single type of model. Such capability is desirable in applications like bidirectional translation},
	annote = {NULL},
	archivePrefix = {arXiv},
	arxivId = {1701.02676},
	author = {Dong, Hao and Neekhara, Paarth and Wu, Chao and Guo, Yike},
	eprint = {1701.02676},
	file = {:F$\backslash$:/Hang/papers/Deep Learning/Generative/conditional/2017{\_}UNIT{\_}Unsupervised Image-to-Image Translation Networks.pdf:pdf},
	mendeley-groups = {Generative/Conditional},
	title = {{Unsupervised Image-to-Image Translation with Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1701.02676},
	volume = {2},
	year = {2017}
}

@article{CycleGAN,
	archivePrefix = {arXiv},
	arxivId = {1703.10593},
	author = {Iccv, Anonymous and Id, Paper},
	eprint = {1703.10593},
	file = {:F$\backslash$:/Hang/papers/Deep Learning/Generative/conditional/2017{\_}CycleGAN{\_}Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks{\_}Iccv, Id.pdf:pdf},
	mendeley-groups = {Generative/Conditional},
	title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}},
	year = {2017}
}

@article{DiscoGAN,
	abstract = {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.},
	archivePrefix = {arXiv},
	arxivId = {1703.05192},
	author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jungkwon and Kim, Jiwon},
	eprint = {1703.05192},
	file = {:F$\backslash$:/Hang/papers/Deep Learning/Generative/conditional/2017{\_}DiscoGAN{\_}Learning to Discover Cross-Domain Relations with Generative Adversarial Networks{\_}Kim et al.pdf:pdf},
	mendeley-groups = {Generative/Conditional},
	title = {{Learning to Discover Cross-Domain Relations with Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1703.05192},
	year = {2017}
}

@article{HowToSketch,
	abstract = {Humans have used sketching to depict our visual world since prehistoric times. Even today, sketching is possibly the only rendering technique readily available to all humans. This paper is the first large scale exploration of human sketches. We analyze the distribution of non-expert sketches of everyday objects such as 'teapot' or 'car'. We ask humans to sketch objects of a given category and gather 20,000 unique sketches evenly distributed over 250 object categories. With this dataset we perform a perceptual study and find that humans can correctly identify the object category of a sketch 73{\%} of the time. We compare human performance against computational recognition methods. We develop a bag-of-features sketch representation and use multi-class support vector machines, trained on our sketch dataset, to classify sketches. The resulting recognition method is able to identify unknown sketches with 56{\%} accuracy (chance is 0.4{\%}). Based on the computational model, we demonstrate an interactive sketch recognition system. We release the complete crowd-sourced dataset of sketches to the community.},
	author = {Eitz, Mathias and Hays, James and Alexa, Marc},
	doi = {10.1145/2185520.2335395},
	file = {:F$\backslash$:/Hang/papers/Sketch/2012{\_}How do humans sketch objects{\_}Eitz, Hays, Alexa.pdf:pdf},
	isbn = {0730-0301},
	issn = {07300301},
	journal = {ACM Transactions on Graphics},
	keywords = {crowd-sourcing,learning,recognition,sketch},
	mendeley-groups = {Sketch,Sketch/others},
	number = {4},
	pages = {1--10},
	title = {{How do humans sketch objects?}},
	url = {http://dl.acm.org/citation.cfm?doid=2185520.2335395},
	volume = {31},
	year = {2012}
}

@article{SketchANet,
	author = {Yu, Qian and Yang, Yongxin and Liu, Feng and Song, Yi-Zhe and Xiang, Tao and Hospedales, Timothy M.},
	doi = {10.1007/s11263-016-0932-3},
	file = {:F$\backslash$:/Hang/papers/Sketch/QMUL/2016{\_}Sketch-a-Net A Deep Neural Network that Beats Humans{\_}Yu et al.pdf:pdf},
	issn = {0920-5691},
	journal = {International Journal of Computer Vision},
	keywords = {convolutional neural,data augmentation,network,sketch,sketch recognition,stroke ordering},
	mendeley-groups = {Sketch/QMUL},
	title = {{Sketch-a-Net: A Deep Neural Network that Beats Humans}},
	url = {http://link.springer.com/10.1007/s11263-016-0932-3},
	year = {2016}
}

@article{Sketch3DShape,
	abstract = {Retrieving 3D models from 2D human sketches has received considerable attention in the areas of graphics, image retrieval, and computer vision. Almost always in state of the art approaches a large amount of "best views" are computed for 3D models, with the hope that the query sketch matches one of these 2D projections of 3D models using predefined features. We argue that this two stage approach (view selection -- matching) is pragmatic but also problematic because the "best views" are subjective and ambiguous, which makes the matching inputs obscure. This imprecise nature of matching further makes it challenging to choose features manually. Instead of relying on the elusive concept of "best views" and the hand-crafted features, we propose to define our views using a minimalism approach and learn features for both sketches and views. Specifically, we drastically reduce the number of views to only two predefined directions for the whole dataset. Then, we learn two Siamese Convolutional Neural Networks (CNNs), one for the views and one for the sketches. The loss function is defined on the within-domain as well as the cross-domain similarities. Our experiments on three benchmark datasets demonstrate that our method is significantly better than state of the art approaches, and outperforms them in all conventional metrics.},
	archivePrefix = {arXiv},
	arxivId = {1504.03504},
	author = {{Fang Wang} and {Le Kang} and {Yi Li}},
	doi = {10.1109/CVPR.2015.7298797},
	eprint = {1504.03504},
	file = {:F$\backslash$:/Hang/papers/Sketch/2015{\_}Sketch-based 3D shape retrieval using Convolutional Neural Networks{\_}Fang Wang, Le Kang, Yi Li.pdf:pdf},
	isbn = {978-1-4673-6964-0},
	issn = {10636919},
	journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	mendeley-groups = {Sketch/others},
	pages = {1875--1883},
	title = {{Sketch-based 3D shape retrieval using Convolutional Neural Networks}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7298797},
	year = {2015}
}

@article{SketchNet,
	author = {Zhang, Hua and Liu, Si and Zhang, Changqing and Ren, Wenqi and Wang, Rui and Cao, Xiaochun},
	file = {:F$\backslash$:/Hang/papers/CVPR2016/POSTER SESSION/Fine Grained Categorization/2016{\_}SketchNet Sketch Classification with Web Images{\_}Zhang et al.pdf:pdf},
	journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	mendeley-groups = {CVPR/CVPR2016,CVPR/CVPR2016/Poster,Sketch,Sketch/others},
	pages = {1105--1113},
	title = {{SketchNet : Sketch Classification with Web Images}},
	year = {2016}
}

@article{SketchMeThatShoe,
	abstract = {We investigate the problem of fine-grained sketch-based image retrieval (SBIR), where free-hand human sketches are used as queries to perform instance-level retrieval of im- ages. This is an extremely challenging task because (i) vi- sual comparisons not only need to be fine-grained but also executed cross-domain, (ii) free-hand (finger) sketches are highly abstract, making fine-grained matching harder, and most importantly (iii) annotated cross-domain sketch-photo datasets required for training are scarce, challenging many state-of-the-art machine learning techniques. In this paper, for the first time, we address all these challenges, providing a step towards the capabilities that would underpin a commercial sketch-based image retrieval application. We introduce a new database of 1,432 sketch- photo pairs from two categories with 32,000 fine-grained triplet ranking annotations. We then develop a deep triplet- ranking model for instance-level SBIR with a novel data augmentation and staged pre-training strategy to allevi- ate the issue of insufficient fine-grained training data. Ex- tensive experiments are carried out to contribute a vari- ety of insights into the challenges of data sufficiency and over-fitting avoidance when training deep networks for fine- grained},
	author = {Yu, Qian and Liu, Feng and Song, Yi-zhe and Hospedales, Timothy M and Loy, Chen Change},
	doi = {10.1109/CVPR.2016.93},
	file = {:F$\backslash$:/Hang/papers/Sketch/QMUL/2016{\_}Sketch Me That Shoe{\_}Yu et al.pdf:pdf},
	journal = {Cvpr},
	mendeley-groups = {Sketch,Sketch/QMUL},
	pages = {799--807},
	title = {{Sketch Me That Shoe}},
	year = {2016}
}

@article{SketchyDatabase,
	abstract = {We present the Sketchy database, the first large-scale collection of sketch-photo pairs. We ask crowd workers to sketch particular photographic objects sampled from 125 categories and acquire 75,471 sketches of 12,500 objects. The Sketchy database gives us finegrained associations between particular photos and sketches, and we use this to train cross-domain convolutional networks which embed sketches and photographs in a common feature space. We use our database as a benchmark for fine-grained retrieval and show that our learned representation significantly outperforms both handcrafted features as well as deep features trained for sketch or photo classification. Beyond image retrieval, we believe the Sketchy database opens up new opportunities for sketch and image understanding and synthesis.},
	author = {Sangkloy, Patsorn and Burnell, Nathan and Ham, Cusuh and Hays, James},
	doi = {10.1145/2897824.2925954},
	file = {:F$\backslash$:/Hang/papers/Sketch/2016{\_}The Sketchy Database Learning to Retrieve Badly Drawn Bunnies{\_}Sangkloy.pdf:pdf},
	isbn = {9781450342797},
	issn = {07300301},
	journal = {ACM Transactions on Graphics},
	keywords = {computing methodologies,concepts,deep learning,image representations,image synthesis,network,siamese,sketch-based image retrieval,triplet network},
	mendeley-groups = {Sketch},
	number = {4},
	pages = {1--12},
	title = {{The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies}},
	url = {http://www.cc.gatech.edu/{~}hays/tmp/sketchy-database.pdf{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2897824.2925954},
	volume = {35},
	year = {2016}
}

@article{Gatys2015,
	abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the con- tent and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. How- ever, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.1,2 Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to sepa- rate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the strik- ing similarities between performance-optimised artificial neural networks and biological vision,3–7 our work offers a path forward to an algorithmic under- standing of how humans create and perceive artistic imagery.},
	archivePrefix = {arXiv},
	arxivId = {1508.06576},
	author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
	doi = {10.1561/2200000006},
	eprint = {1508.06576},
	file = {:F$\backslash$:/Hang/papers/Deep Learning/NeuralStyle/2015{\_}A Neural Algorithm of Artistic Style{\_}Gatys, Ecker, Bethge.pdf:pdf},
	isbn = {2200000006},
	issn = {1935-8237},
	journal = {arXiv preprint},
	keywords = {eural algorithm of artistic,style},
	mendeley-groups = {Style},
	pages = {3--7},
	pmid = {1000200972},
	title = {{A Neural Algorithm of Artistic Style}},
	year = {2015}
}

@article{Johnson2016a,
	abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a $\backslash$emph{\{}per-pixel{\}} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing $\backslash$emph{\{}perceptual{\}} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
	archivePrefix = {arXiv},
	arxivId = {1603.08155},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	doi = {10.1007/978-3-319-46475-6_43},
	eprint = {1603.08155},
	file = {:F$\backslash$:/Hang/papers/Deep Learning/NeuralStyle/2016{\_}Perceptual Losses for Real-Time Style Transfer and Super-Resolution{\_}Johnson, Alahi, Fei-Fei.pdf:pdf},
	journal = {Arxiv},
	keywords = {deep learning,style transfer,super-resolution},
	mendeley-groups = {Style},
	pages = {1--5},
	title = {{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}},
	url = {http://arxiv.org/abs/1603.08155},
	year = {2016}
}

@article{Ulyanov2016,
	abstract = {Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example. However, their methods requires a slow and memory-consuming optimization process. We propose here an alternative approach that moves the computational burden to a learning stage. Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image. The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys{\~{}}et{\~{}}al., but hundreds of times faster. More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions.},
	archivePrefix = {arXiv},
	arxivId = {1603.03417},
	author = {Ulyanov, Dmitry and Lebedev, Vadim and Vedaldi, Andrea and Lempitsky, Victor},
	eprint = {1603.03417},
	file = {:F$\backslash$:/Hang/papers/Deep Learning/NeuralStyle/2016{\_}Texture Networks Feed-forward Synthesis of Textures and Stylized Images{\_}Ulyanov et al.pdf:pdf},
	journal = {CoRR},
	mendeley-groups = {Style},
	title = {{Texture Networks: Feed-forward Synthesis of Textures and Stylized Images}},
	url = {http://arxiv.org/abs/1603.03417},
	year = {2016}
}

@article{CGAN,
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	archivePrefix = {arXiv},
	arxivId = {1411.1784},
	author = {Mirza, Mehdi and Osindero, Simon},
	eprint = {1411.1784},
	file = {:F$\backslash$:/Hang/papers/Deep Learning/Generative/2014{\_}Conditional Generative Adversarial Nets{\_}Mirza, Osindero.pdf:pdf},
	journal = {CoRR},
	pages = {1--7},
	title = {{Conditional Generative Adversarial Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	year = {2014}
}

@article{GAN,
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1406.2661v1},
	author = {Goodfellow, Ij and Pouget-Abadie, J and Mirza, Mehdi},
	eprint = {arXiv:1406.2661v1},
	file = {:F$\backslash$:/Hang/papers/Deep Learning/CNN{\_}不能错过的10篇论文/2014{\_}Generative Adversarial Networks{\_}Goodfellow, Pouget-Abadie, Mirza.pdf:pdf},
	isbn = {1406.2661},
	issn = {10495258},
	journal = {arXiv preprint arXiv: {\ldots}},
	mendeley-groups = {Deep Learning/Most cited},
	pages = {1--9},
	title = {{Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	year = {2014}
}

@article{Reed2016,
	abstract = {Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neu-ral network architectures have been developed to learn discriminative text feature representa-tions. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to gen-erate highly compelling images of specific cat-egories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image model-ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.},
	archivePrefix = {arXiv},
	arxivId = {1605.05396},
	author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
	eprint = {1605.05396},
	file = {:F$\backslash$:/Hang/papers/Deep Learning/Generative/2016{\_}Generative Adversarial Text to Image Synthesis{\_}Reed et al.pdf:pdf},
	journal = {Icml},
	pages = {1060--1069},
	title = {{Generative Adversarial Text to Image Synthesis}},
	year = {2016}
}

@article{Dosovitskiy2014,
	abstract = {We train generative 'up-convolutional' neural networks which are able to generate images of objects given object style, viewpoint, and color. We train the networks on rendered 3D models of chairs, tables, and cars. Our experiments show that the networks do not merely learn all images by heart, but rather find a meaningful representation of 3D models allowing them to assess the similarity of different models, interpolate between given views to generate the missing ones, extrapolate views, and invent new objects not present in the training set by recombining training instances, or even two different object classes. Moreover, we show that such generative networks can be used to find correspondences between different objects from the dataset, outperforming existing approaches on this task.},
	archivePrefix = {arXiv},
	arxivId = {1411.5928},
	author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Tatarchenko, Maxim and Brox, Thomas},
	doi = {10.1109/CVPR.2015.7298761},
	eprint = {1411.5928},
	file = {:F$\backslash$:/Hang/papers/Deep Learning/Generative/2014{\_}Learning to Generate Chairs, Tables and Cars with Convolutional Networks{\_}Dosovitskiy et al.pdf:pdf},
	isbn = {9781467369640},
	issn = {10636919},
	journal = {arXiv preprint arXiv:1411.5928},
	pages = {1--14},
	pmid = {25246403},
	title = {{Learning to Generate Chairs, Tables and Cars with Convolutional Networks}},
	year = {2014}
}

%%% ============================= Have checked =======================================

@article{ImageNet,
	Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
	Title = {{ImageNet Large Scale Visual Recognition Challenge}},
	Year = {2015},
	journal   = {International Journal of Computer Vision (IJCV)},
	doi = {10.1007/s11263-015-0816-y},
	volume={115},
	number={3},
	pages={211-252}
}

@InProceedings{HED,
	author = {"Xie, Saining and Tu, Zhuowen"},
	Title = {Holistically-Nested Edge Detection},
	Booktitle = "Proceedings of IEEE International Conference on Computer Vision",
	Year  = {2015},
}